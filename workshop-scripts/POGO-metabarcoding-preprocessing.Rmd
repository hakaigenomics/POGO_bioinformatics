---
title: "POGO metabarcoding preprocessing"
author: "Evan Morien and Libby Natola"
date: "2025-04-29"
output: html_document
---

## Set up environment 

The first step for setting up our environment is loading the libraries that are used in this pipeline. In some cases, the order that the libraries are loaded matters, so it is best not to modify it. Note that you need to have already installed libraries. Change the pathway we used in `setwd()` to your own working directory. 

We also set the plot themes which dictates general guidelines for plotting clean figures in ggplot.

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE) # options for Rmarkdown only. Tells R to show the code in the final rendered output (eg. in HTML, PDF, or Word)

library(dada2)
library(phyloseq)
library(tidyverse)
library(reshape2)
library(stringr)
library(data.table)
library(broom)
library(ape)
library(qualpalr)
library(viridis)
library(ShortRead)
library(Biostrings)
library(seqinr)

setwd("/mnt/Genomics/Working/libby.natola/home/libby.natola/projects/POGO/")

theme_set(theme_bw())

```

## Preprocessing

We begin the analysis with the preprocessing steps to trim and filter out bases that are primers, errors, or low in quality. 

### File Path Setup

We set the file paths as a variable so we can call each file in turn. You'll have to change the `path` variable to match where you put your own raw data as well as the `pattern` argument of `list.files` to match the extenstions of your raw forward and reverse files. Then we set a variable with the sample names so we can read in and write out files for each sample with different file types without having to specify each file name manually. 

```{r file_paths_setup}
path <- "raw_data" # CHANGE ME to the directory containing the fastq files
head(list.files(path))
fnFs <- sort(list.files(path, pattern="_R1_001.fastq.gz", full.names = TRUE)) #change the pattern to match all your R1 files
fnRs <- sort(list.files(path, pattern="_R2_001.fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1) #change the delimiter in quotes and the number at the end of this command to decide how to split up the file name, and which element to extract for a unique sample name
```


### Create fastq Quality Plots

Next we will make some plots that show us the quality of nucleotide calls over the positions in all the reads of each sample. We run the forward and reverse reads separately. 

In some cases you may have hundreds of samples in your dataset. It's easier to just look at a few (10-50) of the fastq files to get a general sense of how your sequencing run did. In the interest of time of this tutorial we'll only show 4 as examples. The plot outputs show heatmaps of the frequency of each quality score at each base position. The green line shows the mean quality scores, and the orange lines show the quartiles. Generally quality scores above 30 are quite good (99.9% accurate, 1 error in 1000 bases). It's normal to have the quality decline at the end of the read. 

```{r make_quality_plots, message=FALSE}
numplot <- 16 #CHANGE ME to the number of samples you want to include in the quality plot ( can be anywhere from 2 to length(fnFs) ) # plotting 49 quality plots produces a 7x7 plot, which i find useful and easy to read
a <- sample(fnFs, numplot) #randomly select a set of N samples
b <- which(fnFs %in% a) #identify the indices of those samples
plotfnFs <- fnFs[b] #use the indices to create two lists of corresponding forward and reverse files to plot
plotfnRs <- fnRs[b]
plotQualityProfile(plotfnFs) #this plots the quality profiles for each sample
plotQualityProfile(plotfnRs)
```

### Primer detection and removal

The primers we used to amplify these DNA sequences need to be removed to avoid introducing bias or errors in downstream analysis and to ensure we are analyzing only true biological sequences. Unless you're using the exact primers we use here you'll need to change `FWD` and `REV` to your own primer sequences. 

We want to verify first that the correct primers are in the correct orientation so we count how many we find in each possible orientation. First we need to list all the possible orientations, remove the ambiguous bases, and count the number of times each orientation appears in the raw reads. Most of them should be either in FWD.ForwardReads/REV.ReverseReads Forward column or FWD.ReverseReads/REV.ForwardReads in RevComp.

```{r primer_checking}
FWD <- "GGWACWGGWTGAACWGTWTAYCCYCC"  ## CHANGE ME to your forward primer sequence #current primers here are the CO1-Leray primer set used by Hakai
REV <- "TANACYTCNGGRTGNCCRAARAAYCA"  ## CHANGE ME to your reverse primer sequence
allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, trimRight = c(5,5), trimLeft = c(0,0), maxN = 0, multithread = 38, compress = TRUE, matchIDs=TRUE) #removing all reads with Ns in them, as we can't use them in the dada2 pipeline #here we also refer to the quality plots to determine if any read trimming needs to happen. trimming the first or last N bases at this stage can significantly improve read retention later on, if there was an issue with the run that resulted in very low quality bases at the 3' or 5' end of the reads

primerHits <- function(primer, fn) {
  # This funciton counts number of reads in which the primer is found in any orientation
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
index <- 10 #this is the index of the file we want to check for primers, within the lists "fn*s.filtN", it can be any number from 1 to N, where N is the number of samples you are processing
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[index]]), #the index of the sample you'd like to use for this test is used here (your first sample may be a blank/control and not have many sequences in it, be mindful of this)
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[index]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[index]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[index]]))

####OPTIONAL!!!!####
#REV <- REV.orients[["RevComp"]] #IMPORTANT!!! change orientation ONLY IF NECESSARY. see the online dada2 ITS workflow, section "Identify Primers" for details on when and why to do this
```

Now that we have those primers and we are confident they were input correctly, we can remove them from the raw data and output new trimmed files. You will have to install cutadapt on your computer and add your own pathway to the variable `cutadapt` below. 

```{r primer_removal, message=FALSE}
cutadapt <- "/usr/local/bin/cutadapt" # CHANGE ME to the cutadapt path on your machine
system2(cutadapt, args = "--version")

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 

#Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, "-j", 38, # -n 2 required to remove FWD and REV primer from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}
```

And then check that those primers are all removed. Ideally there should be only 0s in the table.

```{r primer_removal_check}
#sanity check, should report zero for all orientations and read sets
index <- 10 #this is the index of the file we want to check for primers, within the lists "fn*s.cut", it can be any number from 1 to N, where N is the number of samples you are processing
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[index]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[index]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[index]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[index]]))

# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_R1_001", full.names = TRUE)) #remember to change this so it matches ALL your file names!
cutRs <- sort(list.files(path.cut, pattern = "_R2_001", full.names = TRUE)) #remember to change this so it matches ALL your file names!

```

### Trim & filter the reads

In this step we trim off low quality ends and filter out low quality reads. We run the primer-removed reads through dada2's `filterAndTrim` function. Setting the variable `truncLen` truncates the read to a certain length, which you should not use if you have a region that is variable in length. You can trim from the right or left of the read, specifying the value you want to trim for the forwards and the reverse files. Here we only trim off the right end, trimming more off the reverse reads as they are of lower quality. You can set a `minLen` minimum length to exclude sequences shorter than a certain length. Dada2 does not handle Ns so we remove all Ns with `maxN` and set the maximum number of expected errors with `maxEE`. We are typically permissive in the `maxEE` parameter set here, in order to retain the maximum number of reads possible. Error correction steps built into the dada2 pipeline have no trouble handling data with this many expected errors. Next, we truncate reads at the first instance of a quality score less than or equal to `truncQ` value. We also remove phix, filter to only reads with matching forward and reverse reads with matching id fields, compress the output to save disk space, and multi-thread to speed up the run. 

We also start a new dataframe named `out` which we will use to track read retention across subsequent steps. 

```{r filter_trim}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, truncLen=c(0,0), trimLeft = c(0, 0), trimRight = c(25,50), minLen = c(110,110),
                     maxN=c(0,0), maxEE=c(4,6), truncQ=c(2,2), rm.phix=TRUE, matchIDs=TRUE,
                     compress=TRUE, multithread=38) 
retained <- as.data.frame(out)
retained$percentage_retained <- retained$reads.out/retained$reads.in*100
write.table(retained, "retained_reads.CO1.filterAndTrim_step.txt", sep="\t", row.names=TRUE, col.names=TRUE, quote=FALSE)
```

### Learn error rates

The next three sections (learn error rates, dereplication, sample inference) are the core of dada2's sequence processing pipeline. Read the dada2 paper and their online documentation (linked at top of this guide) for more information on how these steps work. 

At this point we have filtered out primers and low quality reads and bases. However, there are still errors in the reads made by the sequencer. Luckily, these machines have predictable error rates, and we can model them and distinguish real biological sequence variants (ASVs) from sequencing errors. 

The package DADA2 uses of a parametric error model (err) to learn the error model from your data with the `learnErrors` function.  

```{r learn_errors, message=FALSE}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)

plotErrors(errF, nominalQ=TRUE) 
plotErrors(errR, nominalQ=TRUE) 
```

### Dereplication

Dereplicate the reads, meaning remove duplicates from the sequence files but retain the number of times each sequence is observed.

```{r dereplication, message=FALSE}
#here reads are dereplicated (only unique sequences and the number of times they are observed are retained)
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)

# Name the derep-class objects by the sample names #this is just to ensure that all your R objects have the same sample names in them
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

### Sample inference

Next, dada2 uses the error rates it calculated above and applies these to your data to identify true sequence variants in each sample. 

```{r sample_inference, message=FALSE}
#see dada2 documentation for further details on this process where error rates are used to estimate whether an observed sequence is unique because of technical noise, or real biological variation
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)

dadaFs[[1]]
dadaRs[[1]]
```

### OPTIONAL: remove low-sequence samples before merging

If there were any particularly low quality samples in your data that have no, or very few, remaining reads after filtering, you'll need to remove the samples from your data for the rest of the script to work correctly. Here, we make a list of those samples with fewer than 100 reads that need to be removed.  

`r samples_to_keep <- rownames(out)[as.numeric(out[,"reads.out"]) > 100]` 

### Track reads

Add to read retention data. 

```{r track_reads}
getN <- function(x) sum(getUniques(x)) #keeping track of read retention, number of unique sequences after ASV inference
track <- cbind(sapply(derepFs, getN), sapply(derepRs, getN), sapply(dadaFs, getN), sapply(dadaRs, getN))
samples_to_keep <- track[,4] > 50 #your threshold. try different ones to get the lowest one that will work. #this method accounts for dereplication/ASVs left after inference
samples_to_remove <- names(samples_to_keep)[which(samples_to_keep == FALSE)] #record names of samples you have the option of removing
write.table(names(which(samples_to_keep == TRUE)), "samples_retained.txt", row.names=FALSE, quote=F, sep="\n")
write.table(setdiff(sample.names, names(which(samples_to_keep == TRUE))), "samples_removed.txt", row.names=FALSE, quote=F, sep="\n")
```

### Merge paired reads

Now put the forward and reverse reads together as one contiguous sequence by aligning the forward reads with the reverse-complement of the corresponding reverse reads. Sequences are only merged if the forward and reverse reads overlap by at least 12 identical bases. 

```{r merge_pairs, message=FALSE}
mergers <- mergePairs(dadaFs[samples_to_keep], derepFs[samples_to_keep], dadaRs[samples_to_keep], derepRs[samples_to_keep], verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

#construct sequence table
seqtab <- makeSequenceTable(mergers)
```

### View Sequence Length Distribution Post-Merging

Make sure your sequences are about the length you expect. Most ASVs should be within a few bp of the amplicon length, minus the length of the primers

```{r sequence_length_distn, message=FALSE}
length.histogram <- as.data.frame(table(nchar(getSequences(seqtab)))) #tabulate sequence length distribution
plot(x=length.histogram[,1], y=length.histogram[,2]) #view length distribution plot
```

`r plot(x=length.histogram[,1], y=length.histogram[,2]) #view length distribution plot`

### Remove low-count singleton ASVs and build ASV report

Now we remove ASVs that were only seen in a single sample and account for ≤ 0.1% of all reads in that sample. This is because such reads are probably artifacts, rather than true biological sequences. Then we start constructing a file to write with the number of ASVs per sample. 

```{r remove_singletons}
#create phyloseq otu_table
otus <- otu_table(t(seqtab), taxa_are_rows = TRUE)

#generate counts of sample per ASV
otu_pres_abs <- otus
otu_pres_abs[otu_pres_abs >= 1] <- 1 #creating a presence/absence table
otu_pres_abs_rowsums <- rowSums(otu_pres_abs) #counts of sample per ASV

#IF you want to filter out rare variants (low-read-count singleton ASVs) you can use phyloseq's "transform_sample_counts" to create a relative abundance table, and then filter your ASVs by choosing a threshold of relative abundance: otus_rel_ab = transform_sample_counts(otus, function(x) x/sum(x))
dim(seqtab) # sanity check
dim(otus) # (this should be the same as last command, but the dimensions reversed)

otus_rel_ab <- transform_sample_counts(otus, function(x) x/sum(x)) #create relative abundance table
df <- as.data.frame(unclass(otus_rel_ab)) #convert to plain data frame
df[is.na(df)] <- 0 #if there are samples with no merged reads in them, and they passed the merge step (a possiblity, converting to a relative abundance table produes all NaNs for that sample. these need to be set to zero so we can do the calculations in the next steps.)
otus_rel_ab.rowsums <- rowSums(df) #compute row sums (sum of relative abundances per ASV. for those only present in one sample, this is a value we can use to filter them for relative abundance on a per-sample basis)
a <- which(as.data.frame(otu_pres_abs_rowsums) == 1) #which ASVs are only present in one sample
b <- which(otus_rel_ab.rowsums <= 0.001) #here is where you set your relative abundance threshold #which ASVs pass our filter for relative abundance
removed <- length(intersect(a,b)) #how many of our singleton ASVs fail on this filter
rows_to_remove <- intersect(a,b) #A also in B (we remove singleton ASVs that have a lower relative abundance value than our threshold)
otus_filt <- otus[-rows_to_remove,] #filter OTU table we created earlier
dim(otus_filt) #how many ASVs did you retain?
seqtab.nosingletons <- t(as.matrix(unclass(otus_filt))) #convert filtered OTU table back to a sequence table matrix to continue with dada2 pipeline

#Start ASV report
cat("dim(otus):", dim(otus),file="ASV_report.txt",sep="\t",append=TRUE)
cat("", file="ASV_report.txt", sep="\n", append=TRUE)
cat("# ASVs Removed:", length(intersect(a,b)),file="ASV_report.txt",sep="\t",append=TRUE)
cat("", file="ASV_report.txt", sep="\n", append=TRUE)
cat("dim(otus_filt):", dim(otus_filt),file="ASV_report.txt",sep="\t",append=TRUE)
cat("", file="ASV_report.txt", sep="\n", append=TRUE)
```

### Remove chimeras

Chimeras are sequences formed when fragments from multiple different template sequences are mistakenly joined during PCR amplification. Here, we remove "bimeras", or chimeras with two sources. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences. Look at "method" to decide which type of pooling you'd like to use when judging each sequence as chimeric or non-chimeric.

```{r remove_chimeras}
seqtab.nosingletons.nochim <- removeBimeraDenovo(seqtab.nosingletons, method="pooled", multithread=16, verbose=TRUE) #this step can take a few minutes to a few hours, depending on the size of your dataset
cat("dim(seqtab.nosingletons.nochim):", dim(seqtab.nosingletons.nochim),file="ASV_report.txt",sep="\t",append=TRUE)
cat("", file="ASV_report.txt", sep="\n", append=TRUE)
sum(seqtab.nosingletons.nochim)/sum(seqtab.nosingletons) #proportion of nonchimeras #it should be relatively high after filtering out your singletons/low-count ASVs, even if you lose a lot of ASVs, the number of reads lost should be quite low
cat("proprtion of chimeras:", sum(seqtab.nosingletons.nochim)/sum(seqtab.nosingletons),file="ASV_report.txt",sep="\t",append=TRUE)
```


### Track read retention

Add the data from the pair merging, singleton removal, and chimera removal steps, save sequence table output

```{r tracking}
getN <- function(x) sum(getUniques(x))
track <- cbind(out[samples_to_keep,], sapply(dadaFs[samples_to_keep], getN), sapply(dadaRs[samples_to_keep], getN), sapply(mergers, getN), rowSums(seqtab.nosingletons), rowSums(seqtab.nosingletons.nochim))
# If processing only a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
track <- cbind(track, 100-track[,6]/track[,5]*100, 100-track[,7]/track[,6]*100, track[,7]/track[,1]*100)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nosingletons", "nochimeras", "percent_singletons", "percent_chimeras", "percent_retained_of_total")

####save output from sequnce table construction steps####
write.table(data.frame("row_names"=rownames(track),track),"read_retention.CO1_merged.txt", row.names=FALSE, quote=F, sep="\t")
write.table(data.frame("row_names"=rownames(seqtab.nosingletons.nochim),seqtab.nosingletons.nochim),"sequence_table.CO1.merged.txt", row.names=FALSE, quote=F, sep="\t")
```

**OPTIONAL**, if needed: read in original sequence table with sequences as ASV labels.
This is appropriate/necessary if you are loading in a sequence table produced on a remote machine, or in a separate instance of R.

```{r remote_files, echo=FALSE}
# seqtab.nosingletons.nochim <- fread("sequence_table.CO1.merged.txt", sep="\t", header=T, colClasses = c("row_names"="character"), data.table=FALSE)
# row.names(seqtab.nosingletons.nochim) <- seqtab.nosingletons.nochim[,1] #set row names
# seqtab.nosingletons.nochim <- seqtab.nosingletons.nochim[,-1] #remove column with row names in it
# seqtab.nosingletons.nochim <- as.matrix(seqtab.nosingletons.nochim) #cast the object as a matrix
# mode(seqtab.nosingletons.nochim) <- "numeric"
```

### Save sequences for both ASV tables separately

Replace the long ASV names (the actual sequences) with human-readable names. Save the new names and sequences as a .fasta file in your project working directory, and save a table that shows the mapping of sequences to new ASV names.

```{r ASV_tables}
my_otu_table <- t(as.data.frame(seqtab.nosingletons.nochim)) #transposed (OTUs are rows) data frame. unclassing the otu_table() output avoids type/class errors later on
ASV.seq <- as.character(unclass(row.names(my_otu_table))) #store sequences in character vector
ASV.num <- paste0("ASV", seq(ASV.seq), sep='') #create new names
write.table(cbind(ASV.num, ASV.seq), "sequence_ASVname_mapping.CO1.txt", sep="\t", quote=F, row.names=F, col.names=F)
write.fasta(sequences=as.list(ASV.seq), names=ASV.num, "CO1_ASV_sequences.fasta") #save sequences with new names in fasta format
```

### IMPORTANT: sanity checks

Only proceed if this tests as true for all elements. If the response to `all(colnames(seqtab.nosingletons.nochim) == ASV.seq)` is `TRUE` continue with reformatting the table. 

```{r ASV_tables_cont}
#assign new ASV names
colnames(seqtab.nosingletons.nochim) <- ASV.num

#re-save sequence and taxonomy tables with updated names
write.table(data.frame("row_names"=rownames(seqtab.nosingletons.nochim),seqtab.nosingletons.nochim),"sequence_table.CO1.merged.w_ASV_names.txt", row.names=FALSE, quote=F, sep="\t")
#at this point, you are ready to do BLAST-based taxonomy assignment
```


## Taxonomy assignment

### Taxonomy assignment with blast

This next step is not run from R, but in a terminal window. We can show that script here in Rmarkdown, but note that it cannot be run from R. Here we are using the local alignment tool, `blast`, to assign taxonomy to each ASV. You need to have the program blastn and the ncbi nt database downloaded to your machine to run this code. 

```{bash assign_taxonomy, eval=FALSE} 
mkdir blast_96_sim
# blast against genbank NT blast DB
blastn -task megablast -num_threads 46 -evalue 1e-5 -max_target_seqs 10 -perc_identity 96 -qcov_hsp_perc 50 -db /mnt/Genomics/Working/databases/NCBI_NT/2025-02-27/nt -outfmt '6 qseqid stitle sacc staxid pident qcovs evalue bitscore' -query CO1_ASV_sequences.fasta  -out blast_96_sim/CO1_ASV_sequences.blast.out

#filter input fasta to only contain sequences with no hits in the above two blast runs
cat blast_96_sim/CO1_ASV_sequences*blast.out | cut -f1,1 | sort | uniq > blast_96_sim/blast_hit_ASVs.txt
grep -wv -f blast_96_sim/blast_hit_ASVs.txt sequence_ASVname_mapping.CO1.txt | cut -f1,1 | sort > blast_96_sim/no_blast_hit_ASVs.txt
awk -F'>' 'NR==FNR{ids[$0]; next} NF>1{f=($2 in ids)} f' blast_96_sim/no_blast_hit_ASVs.txt CO1_ASV_sequences.fasta > blast_96_sim/CO1_ASV_sequences.no_blast_hit.fasta

#blast this output with lower thresholds for similarity
mkdir blast_90_sim
# blast against genbank NT blast DB
blastn -task megablast -num_threads 46 -evalue 1e-5 -max_target_seqs 10 -perc_identity 90 -qcov_hsp_perc 50 -db /mnt/Genomics/Working/databases/NCBI_NT/2025-02-27/nt -outfmt '6 qseqid stitle sacc staxid pident qcovs evalue bitscore' -query blast_96_sim/CO1_ASV_sequences.no_blast_hit.fasta  -out blast_90_sim/CO1_ASV_sequences.blast.out
```

### Add taxonIds in R

**Back in R**, we need to add taxonIDs for the customDB (adding them directly to the blast DB has not worked in the past, they don't get returned in the blast output). Using the blast output and a map of accessions to taxonIDs, we add the taxonIDs for each blast result.

```{r combine_blast_results, eval=FALSE}
#combine blast results in a way that the add taxonomy and LCA scripts can handle
blastout_NCBINT <- read.delim("blast_96_sim/CO1_ASV_sequences.blast.out", sep="\t", header=F)
blastout_NCBINT_2 <- read.delim("blast_90_sim/CO1_ASV_sequences.blast.out", sep="\t", header=F)
blastout_NCBINT <- rbind(blastout_NCBINT, blastout_NCBINT_2) #join iterations for NT blast
tmp <- blastout_NCBINT[order(-blastout_NCBINT$V5),] #order descending order for percent identity
tmp <- tmp[order(tmp$V1),] #order by ASV name
blastout_NCBINT <- tmp 
#write to file
write.table(blastout_NCBINT, "blast_96_sim/CO1_ASV_sequences.combined_all.blast.out", row.names=F, col.names=F, quote=F, sep="\t")
```

### Finish taxonomy assignment

Now that we have all the taxonIDs, we can finish running the taxonomy assignment with blast and LCA. 


```{bash, eval=FALSE}
#avoid " keyerror: 'NA' " with python script by filtering on "NA" as a whole word string
#explanation: occasionally, an output line from blast has an NA, which causes an error with the Simple-LCA script below. quick fix is to remove these lines (they're quite rare anyway)
grep -v -w "NA" blast_96_sim/CO1_ASV_sequences.combined_all.blast.out > blast_96_sim/tmp

#execute first step for the LCA program (adding taxonomy strings based on taxonIDs in blast output)
python2 /mnt/Genomics/Working/programs/galaxy-tool-BLAST/blastn_add_taxonomy_lite.py -i blast_96_sim/tmp -t /mnt/Genomics/Working/databases/NCBI_taxonomy/2025-02-27/rankedlineage.dmp -m /mnt/Genomics/Working/databases/NCBI_taxonomy/2025-02-27/merged.dmp -o taxonomy
cat <(head -n 1 /mnt/Genomics/Working/programs/galaxy-tool-lca/example/example.tabular) taxonomy_tmp > tmp #here we need to add the header from the example table in order for the lca_species script to work (see below, after taxonomy string corrections)

#in this section, a series of taxonomy string modifications are made. This makes the final output more readable/easier to understand, but is optional.
#IMPORTANT: please note that the filtering criteria in the final step depend on some of this filtering (e.g. blast hits with the word "phylum" will be removed. see -fh parameter in final step) 
# because i'm replacing "unknown phylum" in these sed commands, these sequences are retained.
# if you choose not to do the replacement, the blast hits with "unknown plylum" will not be used in LCA determination unless you also change the filtering criteria set in the -fh parameter during the final step.
# also note, "unknown phylum" is present in any taxonomy where the clade's phylum is uncertain in the NCBI taxonomy system, it doesn't indicate any other kind of uncertainty about the provenance of the sequence.

#label fix for clades missing "kingdom" label
sed -i 's/unknown kingdom \/ Bacillariophyta/Eukaryota \/ Bacillariophyta/g' tmp #Bacillariophyta
sed -i 's/unknown kingdom \/ Ciliophora/Eukaryota \/ Ciliophora/g' tmp #Ciliophora
sed -i 's/unknown kingdom \/ Discosea/Eukaryota \/ Discosea/g' tmp #Discosea
sed -i 's/unknown kingdom \/ Evosea/Eukaryota \/ Evosea/g' tmp #Evosea
sed -i 's/unknown kingdom \/ Haptista/Eukaryota \/ Haptista/g' tmp #Haptista
sed -i 's/unknown kingdom \/ Rhodophyta/Eukaryota \/ Rhodophyta/g' tmp #Rhodophyta

#and for those missing kingdom + phylum labels
sed -i 's/Eukaryota \/ unknown phylum \/ Chrysophyceae/Eukaryota \/ Chrysophyceae \/ Chrysophyceae/g' tmp #Chrysophyceae
sed -i 's/Eukaryota \/ unknown phylum \/ Cryptophyceae/Eukaryota \/ Cryptophyceae \/ Cryptophyceae/g' tmp #Cryptophyceae
sed -i 's/Eukaryota \/ unknown phylum \/ Oomycota/Eukaryota \/ Oomycota \/ Oomycota/g' tmp #Oomycota
sed -i 's/Eukaryota \/ unknown phylum \/ Phaeophyceae/Eukaryota \/ Phaeophyceae \/ Phaeophyceae/g' tmp #Phaeophyceae
sed -i 's/Eukaryota \/ unknown phylum \/ Phaeophyceae/Eukaryota \/ Phaeophyceae \/ Phaeophyceae/g' tmp #Phaeophyceae
sed -i 's/Eukaryota \/ unknown phylum \/ Bigyra/Eukaryota \/ Bigyra \/ Bigyra/g' tmp #Bigyra
sed -i 's/Eukaryota \/ unknown phylum \/ Dictyochophyceae/Eukaryota \/ Dictyochophyceae \/ Dictyochophyceae/g' tmp #Dictyochophyceae
sed -i 's/Eukaryota \/ unknown phylum \/ Dinophyceae/Eukaryota \/ Dinophyceae \/ Dinophyceae/g' tmp #Dinophyceae
sed -i 's/Eukaryota \/ unknown phylum \/ Pelagophyceae/Eukaryota \/ Pelagophyceae \/ Pelagophyceae/g' tmp #Pelagophyceae
sed -i 's/Eukaryota \/ unknown phylum \/ Raphidophyceae/Eukaryota \/ Raphidophyceae \/ Raphidophyceae/g' tmp #Raphidophyceae
sed -i 's/Eukaryota \/ unknown phylum \/ Synurophyceae/Eukaryota \/ Synurophyceae \/ Synurophyceae/g' tmp #Synurophyceae
sed -i 's/Eukaryota \/ unknown phylum \/ Bolidophyceae/Eukaryota \/ Bolidophyceae \/ Bolidophyceae/g' tmp #Bolidophyceae
sed -i 's/Eukaryota \/ unknown phylum \/ Polycystinea/Eukaryota \/ Polycystinea \/ Polycystinea/g' tmp #Polycystinea
sed -i 's/Eukaryota \/ unknown phylum \/ Choanoflagellata/Eukaryota \/ Choanoflagellata \/ Choanoflagellata/g' tmp #Choanoflagellata
sed -i 's/Eukaryota \/ unknown phylum \/ Filasterea/Eukaryota \/ Filasterea \/ Filasterea/g' tmp #Filasterea

#label for those missing kindom + phylum + class labels
sed -i 's/Eukaryota \/ unknown phylum \/ unknown class \/ Telonemida/Eukaryota \/ Telonemida \/ Telonemida \/ Telonemida/g' tmp #Telonemida
sed -i 's/Eukaryota \/ unknown phylum \/ unknown class \/ Jakobida/Eukaryota \/ Jakobida \/ Jakobida \/ Jakobida/g' tmp #Jakobida
sed -i 's/Eukaryota \/ unknown phylum \/ unknown class \/ Pirsoniales/Eukaryota \/ Pirsoniales \/ Pirsoniales \/ Pirsoniales/g' tmp #Pirsoniales

#execute final step for the LCA program (forming consensus taxonomies for each ASV)
mv -f tmp blast_96_sim/CO1_ASV_sequences.combined_all.blast.out #put tmp file where it belongs, add label (this is an overwrite, careful!!)
python2 /mnt/Genomics/Working/programs/galaxy-tool-lca/lca.species.py -i blast_96_sim/CO1_ASV_sequences.combined_all.blast.out -o taxonomy_table.CO1.NCBI_NT+customDB.iterative_blast.LCA+best_hit.txt -b 100 -id 90 -cov 50 -t best_hit -tid 98 -tcov 50 -fh environmental,unidentified,kingdom -flh unclassified
python2 /mnt/Genomics/Working/programs/galaxy-tool-lca/lca.species.py -i blast_96_sim/CO1_ASV_sequences.combined_all.blast.out -o taxonomy_table.CO1.NCBI_NT+customDB.iterative_blast.LCA_only.txt -b 100 -id 90 -cov 50 -t only_lca -fh environmental,unidentified,kingdom -flh unclassified

#cleanup
rm taxonomy_tmp blast_96_sim/tmp #remove redundant files
```

